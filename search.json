[
  {
    "objectID": "index.html#about-this-guide",
    "href": "index.html#about-this-guide",
    "title": "Health Data Science in Aotearoa New Zealand: A Practical Guide",
    "section": "About this guide",
    "text": "About this guide\nThis guide is a collection of learnings based on the experience of Aotearoa New Zealand health data science researchers and other contributors. Aimed at practitioners and those that oversee their work, the guide is intended to provide practical advice without repeating the content of publicly available resources and references; instead, links to these are provided. For data scientists, the guide also assumes a basic understanding of data science techniques, addressing the context and principles of how these are applied to lead to a successful outcome.\nEarlier sections outline the context and setup of a health data science project; later sections introduce the technical aspects of data science including modelling.\nPlease contribute to this guide to share your knowledge and questions with the wider health data science research community. It is a living document and contributions from all viewpoints are most welcome. You can contribute directly by editing a page using the “Edit this page” link in the right sidebar, or by working with the page source directly. Pages are Markdown-formatted text files in a public GitHub repository rendered using Quarto. Alternatively, you can send suggestions through email to info at precisiondrivenhealth.com."
  },
  {
    "objectID": "index.html#about-precision-driven-health",
    "href": "index.html#about-precision-driven-health",
    "title": "Health Data Science in Aotearoa New Zealand: A Practical Guide",
    "section": "About Precision Driven Health",
    "text": "About Precision Driven Health\nPrecision Driven Health (PDH) is partnership between Aotearoa New Zealand’s health information technology (IT) sector, health providers and universities, aimed at improving health outcomes through data science.\nPDH seeks to increase data science capability in New Zealand’s health sector and encourage innovation in the use of health data."
  },
  {
    "objectID": "index.html#contributors-and-reviewers",
    "href": "index.html#contributors-and-reviewers",
    "title": "Health Data Science in Aotearoa New Zealand: A Practical Guide",
    "section": "Contributors and reviewers",
    "text": "Contributors and reviewers\nWe would like to thank the people who have contributed to compiling and reviewing this guide.\n\nAlex Kazemi\nCK Jin\nDuncan Croft\nEdmond Zhang\nFleur Armstrong\nIvan Rivera\nJamal Zolhavarieh\nJuliet Rumball-Smith\nKelly Atkinson\nKevin Ross\nLuke Boyle\nNing Hua\nPieta Brown\nQuan Sun\nRachel Owens\nTom Gutteridge\nVipula Dissanayake"
  },
  {
    "objectID": "02-starting.html",
    "href": "02-starting.html",
    "title": "1  How do I start?",
    "section": "",
    "text": "No matter how simple your idea, it will involve more than just a spreadsheet or computer code. You’ll need to understand the question to answer, or the clinical concept that you want to address. Getting access to appropriate data to help drive your insights can often be a challenge, too.\nMuch like other aspects of business or research, you’ll probably need to convince at least one other person that your idea is important and worthy of the time or resources you need to pursue it. After navigating these hurdles and producing a model or other analysis, your carefully crafted tool will need care and periodic review to stay functional and relevant.\nIf you are coming from a clinical background, you might appreciate some of the context around understanding data identifiability (?sec-identifiability), and its preparation for modelling.\nIf you are approaching this from a data science background, the unique health data landscape ?sec-landscape will be important to appreciate.\nIf you are looking for a more general introduction to health data science in general, keep reading from here!"
  },
  {
    "objectID": "03-basics.html#data-projects-have-a-lifecycle",
    "href": "03-basics.html#data-projects-have-a-lifecycle",
    "title": "2  Understanding the basics",
    "section": "2.1 Data projects have a lifecycle",
    "text": "2.1 Data projects have a lifecycle\nExisting data science guides often include the concept of a data science lifecycle. Examples include the CRoss Industry Standard Process for Data Mining (CRISP-DM) and the Microsoft Team Data Science Process (TDSP).\n\n\n\nThe CRISP-DM Data Science Lifecycle\n\n\nThis guide aligns broadly with the life cycle stages below, but your project may follow a different path. We’ve also included additional considerations around structuring and resourcing a health data science project."
  },
  {
    "objectID": "03-basics.html#the-data-science-life-cycle-stages",
    "href": "03-basics.html#the-data-science-life-cycle-stages",
    "title": "2  Understanding the basics",
    "section": "2.2 The data science life cycle stages",
    "text": "2.2 The data science life cycle stages\n\nBusiness understanding – What does the health system or business or world need?\nData acquisition and understanding – What data do we have/need? Is it ‘clean’?\nData preparation – How do we organise the data for analysis, including modelling?\nModelling – What modelling techniques should we apply?\nEvaluation – Which model best meets the health system or business or world’s objectives?\nDeployment – How do users and stakeholders access the results?\nMaintenance - How will a model be monitored and refreshed over time?\n\nThe Virtual Health Information Network also provides guides of this nature which relate to the Integrated Data Infrastructure."
  },
  {
    "objectID": "03-basics.html#consider-the-business-case",
    "href": "03-basics.html#consider-the-business-case",
    "title": "2  Understanding the basics",
    "section": "2.3 Consider the business case",
    "text": "2.3 Consider the business case\n\n2.3.1 Research vs audit vs ‘business as usual’\nPeople and organisations undertaking health data science may need help clarifying whether what they want to do is “research” or not. There are many definitions of research, but a simple definition is:\nResearch creates new knowledge, which could include new methods or processes, and could lead to the creation of new guidelines.\nBusiness as usual (BAU) includes activities such as development, sales, and support. If your business includes the creation of new capabilities or features, this portion of your activities can be perceived as research.\nAudits are activities that check whether you are following existing processes or guidelines. Testing new processes becomes research, since you are departing from the existing processes.\nQuality improvement seeks to improve an existing process that is already of benefit to patients. Machine learning (ML) models incorporated into workflows can cause changes to the standard of care patients receive, including unintended consequences. Translation from a tool to implementation is essentially research to validate and determine the impact of ML on patients.\nResearch projects can have varying goals, such as ‘analysis only’ versus ‘analyse and model for future use’. Some projects may have a commercial focus on delivering an outcome for use in practice. It is important to define what you are doing as clearly as possible at the start of a project (see Social licence & consent).\n\n\n\n\n\n\nNote\n\n\n\nAll research undertaken in New Zealand using health data requires ethical review (see Ethics and privacy)."
  },
  {
    "objectID": "03-basics.html#clearly-articulate-the-goal",
    "href": "03-basics.html#clearly-articulate-the-goal",
    "title": "2  Understanding the basics",
    "section": "2.4 Clearly articulate the goal",
    "text": "2.4 Clearly articulate the goal\nGood data science doesn’t have to be complicated, but it should be clear. Many projects fail to clearly articulate their goal which may lead to people working to a different agenda.\nCo-design - a design-led process that uses creative and participatory methods involving all stakeholders to ensure the result meets their needs - is essential at this early stage. It’s important to be explicit in these circumstances; you cannot define a problem, determine the appropriate data and methodology, interpret results or run a successful project without the input and partnership of the end-users and other stakeholders.\nDefine goals up front, and consider what question is being answered: Do you want to answer a question that is relevant to a specific population (often geographically defined), or to produce a model or other output that can be generalised to other populations? Is it a proof of concept where further work may be required, or does it require implementation to be used in practice?\nOften the real goal is masked by sub-goals. A project that has been forced to fit within a call for proposals or programme, but where the actual goals of the researcher and those that the project is set up to address are different, is one example of this. Carefully consider the problem you’re trying to solve and seek external validation to confirm if it’s a real-world problem for end users."
  },
  {
    "objectID": "03-basics.html#consider-equity",
    "href": "03-basics.html#consider-equity",
    "title": "2  Understanding the basics",
    "section": "2.5 Consider equity",
    "text": "2.5 Consider equity\nEquity (the quality of being fair and impartial) is considered throughout this guide. When setting up a project, plan for this in each step of the process. When this is not considered upfront, overall gains come at the expense of differences in equity.\nA constant view of equity, also called an “equity lens”, is also addressed throughout this guide, and co-design is central to this concept."
  },
  {
    "objectID": "03-basics.html#choosing-the-right-question---exploring-feasibility-and-delivering-value",
    "href": "03-basics.html#choosing-the-right-question---exploring-feasibility-and-delivering-value",
    "title": "2  Understanding the basics",
    "section": "2.6 Choosing the right question - exploring feasibility and delivering value",
    "text": "2.6 Choosing the right question - exploring feasibility and delivering value\nChoosing the right question is an essential part of any research effort.\nBefore beginning, get really clear on the problem that is being solved and understand if it is clinically meaningful and adds value. Define the need or problem and then find data/technology to answer it, not the other way around. For model development, define what success looks like e.g how accurate should a model be.\n\n\n\n\n\n\nNote\n\n\n\nCo-design is essential - seek information from different stakeholders (clinicians, patients/consumers, systems users) and research the existing evidence/base or literature on the topic. Simple frameworks like the Five Whys can be helpful to apply. The end-users and those impacted by the use of a tool need to understand and believe in the value being delivered to drive engagement and improve translation into real world applications.\n\n\nMapping the clinical workflow into which a model fits in dynamic contexts is also useful - do clinicians have current workarounds for problems or barriers that remain useful, or is there a genuine gap ? Can the model or algorithm be responsive to real world complexity?\nConsider the wider health sector and if there is willingness and need for uptake. Value can be provided through: improvement in patient outcomes or experience, improving efficiency, reduction of uncertainty, prioritising resources to those most in need, reduction in resource requirements.\nIn building a business case, the value of data science projects needs to be expressed from the perspectives of each of the different stakeholders. Consider the unique selling points, unintended consequences, risks vs benefits from social, economic, health outcomes and reputational perspectives. Articulate value based on each stakeholder’s perspective. Consider engaging with a health economist for specialist advice. Do not exaggerate or over-sell the potential real-life impact or underplay potential risks. (These are discussed more in the Governance section.)\n\n\n\n\n\n\nNote\n\n\n\nWork through the use-case end-to-end before starting, getting clear on how the algorithm will be accessed and used, and what decisions will be made as a result.\n\n\nWhat level of transparency or interpretability (REF transparency section) is required in order for the results to be useful? Do we only care about predictions or are we looking to drive process change?\nConsider how the work will be validated or trialled (REF evaluation section) in a clinical setting, e.g. as a clinical trial or passive background comparison with production data.\nAlgorithms and models are only one piece of the puzzle in improving health outcomes. What are the interventions in place or available to take action based on machine learning-driven insights? For example, if someone is identified as being at higher risk of readmission to hospital,what can be offered as a result?\nIs an algorithm necessarily the best solution? Consider guidance to avoid overuse and misuse of machine learning in clinical research (Volovici et al. 2022).\n\nVolovici, V, Syn, N L, Ercole, A, Zhao, J J, and Liu, N. 2022. “Steps to avoid overuse and misuse of machine learning in clinical research”. Nature Medicine 28.10, 1996–1999. https://www.nature.com/articles/s41591-022-01961-6."
  },
  {
    "objectID": "03-basics.html#funding",
    "href": "03-basics.html#funding",
    "title": "2  Understanding the basics",
    "section": "2.7 Funding",
    "text": "2.7 Funding\nFinding funding to pursue a data science project can be difficult. In some cases, data science endeavours may be supported by a specific public or private organisation. Recognising and declaring conflicts of interest is essential to understanding the applicability of the data science results, and any biases that may apply.\nIn New Zealand, funding tends to be available for conceptual research, or for commercialisation activities. In our experience, there is substantial translational work required between these two phases. Potential sources for funding translative work include Callaghan Innovation and the MedTech Innovation Quarter (MedTech-iQ), as well as private companies who invest in research and development activities. Note that the NZ government’s Research and Development Tax Incentive scheme (RDTI) may incentivise these funding activities where the path to commercialisation is less certain.\nIt is usually necessary to pitch, propose, or otherwise justify a request for funding data science activities. You might be responding to a third party’s request for proposals (RFP) or open call for a funding round. When considering the project you want to do, think about the points below.\n\nWho is paying for this work to be completed? What is their interest?\nWho else has an interest in the outcome, including the researchers? Does their influence/interest in the outcome need to be declared or managed in any particular way?\nIf this work is successful, what further investment will be required to ensure that the work leads to its full potential?\nIf the ultimate goal is a commercial product, there is often substantial investment required to take a proof of concept to a maintainable, robust product. Think about:\n\nWho will provide support to the people using this product, even if they are using it only for research purposes or in informal ways?\nWhat is the lifecycle of the research output? Who will be able to look after it in 5 years’ time?"
  },
  {
    "objectID": "03-basics.html#legal-ip-and-regulatory-considerations",
    "href": "03-basics.html#legal-ip-and-regulatory-considerations",
    "title": "2  Understanding the basics",
    "section": "2.8 Legal, IP, and regulatory considerations",
    "text": "2.8 Legal, IP, and regulatory considerations\nLegal advice should be sought, and issues related to legal, intellectual property and/or regulatory issues explicitly discussed and documented prior to you starting work. Consider who holds the data you want to use, who owns the models and what may be required for lifecycle management.\nSoftware may be considered a medical device if it is used in the diagnosis, treatment, prevention, cure, or mitigation of diseases or other conditions. Depending on how the intended use is defined, the software may be subject to country dependent regulatory requirements. Specialised regulatory advisors should be engaged early as the process takes time and documentation for approval may need to be generated during the development process."
  },
  {
    "objectID": "04-collaboration.html#team-capability",
    "href": "04-collaboration.html#team-capability",
    "title": "3  People, capability & collaboration",
    "section": "3.1 Team capability",
    "text": "3.1 Team capability\nTypical health data science projects require a multidisciplinary team. The involvement of different roles will likely fluctuate through the different stages of a project lifecycle.\nSome individuals will have multiple skills, but few will have everything they need. Commonly needed skills in the team, or accessible through other organisations, include:\n\nsubject matter expertise in data\nmachine learning\nsystem design\nuser interface design\nclinical\ngovernance\nconsumer/patient and specifically impacted communities\nlegal and privacy\nethics\nimplementation and change management.\n\nYou’ll often be involved in multiple projects, so it’s useful to share expertise across projects.\n\n\n\n\n\n\nTip\n\n\n\nIt’s particularly important to allow plenty of time for clinical input to data understanding and preparation for data science.\n\n\nDefining input features in clinically relevant ways is particularly important for health data science. For example, ‘cancer’ might be an important input at an individual level; does this mean currently active cancer? Within the last five years? Any exclusions? Time needs to be allocated to work through and validate these definitions with clinicians.\nAdvisory groups are a good way to elicit feedback. Early in any project try to meet with a group of experts and discuss your research plan. Experts will always have different perspectives on the research and the context in which it will be used (see End-user engagement (3.2), which also covers Māori engagement and co-design)."
  },
  {
    "objectID": "04-collaboration.html#sec-end-user-engagement",
    "href": "04-collaboration.html#sec-end-user-engagement",
    "title": "3  People, capability & collaboration",
    "section": "3.2 End-user engagement",
    "text": "3.2 End-user engagement\nYou should engage end users (usually consumers and clinicians) early in co-design, to understand how outputs can be tangible for those who will use them or be impacted by them (see Transparency, interpretability, and explanation (9.4)). It’s important to understand workflows and where tools and/or models may be used. End-user engagement will also help to drive IT implementation if the benefits are clearly articulated to the appropriate stakeholders so work can be prioritised.\n\n3.2.1 Impacted customers/patients/groups\nData scientists are unlikely to have the correct context or cultural awareness to fully grasp what the data is telling them. This includes Māori and other ethnicity groups, consumers, and perspectives that cover age and ability ranges.\n\n\n\n\n\n\nNote\n\n\n\nAny recommendations for changes or improvements should be interpreted through the lens of the groups they are likely to affect, acknowledging the principle of “nothing about us without us”.\n\n\nHQSC’s Code of expectations for health entities’ engagement with consumers and whānau is a useful framework to bear in mind.\n\n\n3.2.2 Clinicians\nCo-designing from the start and having a clinical champion/sponsor to ensure that developments can be incorporated in existing workflows so they can be used will set you up for success (see Operational deployment (10.1)).\n\n\n\n\n\n\nTip\n\n\n\nClinical engagement often makes or breaks a project - both to ensure efficacy and to advocate and promote use of the model in practice. In general, clinicians will adopt tools and models that solve a genuine problem, fit into their workflow patterns, and save them time or enhance safety.\n\n\nInterpretability (which is defined as understanding the reasoning behind predictions and decisions made by the model) is also key. Important stakeholders are unlikely to have lots of time to learn about your work, so displaying it in an easy-to-understand way is essential. You shouldn’t assume that everyone will interpret outputs in the same way, and understand what action is then required. Keep in mind that prospective customers may not be technologically or mathematically savvy, too."
  },
  {
    "objectID": "04-collaboration.html#collaboration",
    "href": "04-collaboration.html#collaboration",
    "title": "3  People, capability & collaboration",
    "section": "3.3 Collaboration",
    "text": "3.3 Collaboration\n\nAgree collaboration will be be supported across different organisations (who may all be using different software and systems and have limitations around what can be used).\nRole definition, responsibilities\nRegular check-ins/stand-ups are helpful\nBe mindful of individuals’ schedules and availability, particularly clinicians."
  },
  {
    "objectID": "04-collaboration.html#measuring-success",
    "href": "04-collaboration.html#measuring-success",
    "title": "3  People, capability & collaboration",
    "section": "3.4 Measuring success",
    "text": "3.4 Measuring success\n\nBe clear what success looks like. This will be different for research vs. implementation.\n\nAre you looking to learn/measure something specific?\nAre you expecting secondary or downstream benefits\n\nMaking sure the bigger picture is kept in mind - not getting lost in the detail in a way that doesn’t add value\nHave you reached the point of diminishing returns for investing further in model development?\nIf you have measurable benefits as an objective, you should develop a benefits realisation plan."
  },
  {
    "objectID": "04-collaboration.html#project-management",
    "href": "04-collaboration.html#project-management",
    "title": "3  People, capability & collaboration",
    "section": "3.5 Project management",
    "text": "3.5 Project management\n\n\n\n\n\n\nTip\n\n\n\nData science is an iterative process of development. You’ll progressively learn more about the data, relationships in the data and how effective modelling is.\n\n\nGood project management or product development practices should be applied to data science projects, while accepting that these projects are often experimental or exploratory in nature. Some important project management considerations include:\n\nData issues flow ‘downstream’ and can impact every other part of a project. Ensure that sufficient time is allocated upfront to review and correct data quality (this also often happens in cycles - the analysis reveals quirks in the data that can be explored further and corrected)\nProject management should strive for continuous visibility, demonstrable progress. How are we tracking against timelines, budget, and the desired outcome?\nCreate open feedback channels where possible. Think about how to develop mockups or prototypes as early as possible for feedback - can you start with a very simple model, Excel dashboard or static design to help ensure that what you are developing will deliver value?\nHealthcare data science projects will often span multiple organisations\nDocumenting failures and lessons learnt helps prevent repeating mistakes.\nUse technology for progress tracking. Some options include:\n\nJira,\na spreadsheet-based activity tracker,\nTrello board,\nemailed summaries of actions and next steps"
  },
  {
    "objectID": "04-collaboration.html#governance",
    "href": "04-collaboration.html#governance",
    "title": "3  People, capability & collaboration",
    "section": "3.6 Governance",
    "text": "3.6 Governance\nGood governance is fundamentally concerned with the value and risk of a project, and must be established to ensure accountability and oversight. Developing policies and procedures can help you manage risk and clearly articulate principles of accountability, transparency, ethical use, privacy, and consent.\nYou should define and tailor a governance approach based on the needs of your own organisation.\nUsually a data science project is undertaken within a wider programme of work, which has its own governance structure and processes. Occasionally, more substantial initiatives will require their own standalone governance.\nGovernance may be applied at different levels, such as governance of data (inputs) or governance of models (product/output). People involved in model governance should come from diverse demographic and technical backgrounds, including perspectives of consumers and/or those who are impacted by the outputs of your work. People involved in data governance need to have an understanding of data flows - how data is captured, stored and used.\nQuestions to ask:\n\nAre ethics applications required?\nWhat existing governance groups and/or processes would need to be involved?\nWhich policies need to be followed?\nWhat level of documentation is required throughout?\nWill this be shared publicly? If so, how?\nWho is responsible for signing off?\nIs a new governance structure or process required here?\nWhat maintenance and/or ongoing review may be required? The NICE Evidence Standards Framework for Digital Health Technologies has a helpful list of points to consider.\n\nThere are many resources for governance groups and executive teams who are looking to write data policies and procedures. These include:\n\nNew Zealand Data and Information Management Principles (Digital.govt.nz): The government’s open data policies and best-practice guidance for agencies managing how data is stored, published and used\nNational Ethical Standards for Health and Disability Research (NEAC): These standards set out the ethical requirements for researchers, health service providers and disability service providers and apply whether or not research or quality improvement activities require review by an ethics committee\nPrivacy Act 2020: the legal basis for NZ organisations to work with data safely - also see Ethics & privacy (4.3)\nHealth Information Privacy Code 2020 - also see Ethics & privacy (4.3)\nHISO 10064:2017 Health Information Governance Guidelines (Manatū Hauora Ministry of Health): Guidance to the health and disability sector on the safe sharing of health information\nNew Zealand Government Open Access and Licensing framework (NZGOAL) (Data.govt.nz): For those who work for a government agency and want to enable appropriate re-use of your agency’s material by licensing its copyright works or releasing non-copyright material (such as open data) for re-use\nA Path to Social License: Guidelines for Trusted Data Use (Data Futures Partnership): August 2017 summary with continuing relevance around what New Zealand people expect from guidelines for data use and sharing\nLayered model for AI governance(Harvard University): A conceptual framework for thinking about governance for AI.\nLessons learned from developing a COVID-19 algorithm governance framework in Aotearoa New Zealand: Practical considerations from a governance group\nAlgorithm Charter (Data.govt.nz): A set of principles which have been signed off by most NZ government organisations\nHealth and Disability Ethics Committees: the health organisation responsible for reviewing and approving research using health data and/or health system users - also see Ethics & privacy (4.3)\nTe Mana Rauranga: a network of experts who have collected principles for Māori data sovereignty - also see Data sovereignty (4.4)\nManatū Hauora Ministry of Health - Emerging Health Technology Advice & Guidance"
  },
  {
    "objectID": "05-data-landscape.html#finding-data",
    "href": "05-data-landscape.html#finding-data",
    "title": "4  The unique health data landscape",
    "section": "4.1 Finding data",
    "text": "4.1 Finding data\nData for health data science projects is everywhere! There’s no shortage of available data in New Zealand. However, keep the following concepts in mind when you are considering how to find data, or access data you may already be interested in:\n\nData can only be used for the purpose for which it was collected; any other use is called “secondary purpose” and requires additional consent (see Use and re-use of data section)\nHealth data used for research purposes needs thought around how the research will be conducted efficiently, ethically, and with privacy and safety front-of-mind.\n\nTo understand population trends and context, aggregated data sets and web tools are publicly available via the Ministry of Health and Statistics New Zealand (Stats NZ). Micro data (at the level of the individual) is available to researchers on application to National Collections or Stats NZ for Confidentialised Unit Record Files (CURFs). Dissemination of micro data is in accordance with the Privacy Act, health legislation and contracts and access is strictly controlled according to use.\nUseful resources:\n\nThrough a review of Aotearoa New Zealand health datasets, PDH has produced an interactive and updatable list of data available in New Zealand. This includes data from Figure.nz and many other sources.\nWhere can I find health information? (Manatū Hauora): Data sources commonly used when analysing the health of New Zealand populations\nHealth statistics and data sets (Manatū Hauora)\n\n\n4.1.1 The Integrated Data Infrastructure\nSupported by Statistics NZ, the Integrated Data Infrastructure (IDI) is a repository of individual-level data from multiple government sources, able to be linked together, anonymised, and used for research. It’s a massive resource which is accessible through a prescribed process with very high safety and privacy requirements. It may not be possible to derive individual-level insights from IDI data.\nThe Virtual Health Information Network includes many researchers who are using the IDI. VHIN’s IDI guides can help you understand what centralised data is available and how it could be used in your project.\nIf you’re interested in what you could do with the IDI, it’s best to make contact with a researcher who already has experience in using this platform. Refer to the list of research using Stats NZ microdata."
  },
  {
    "objectID": "05-data-landscape.html#understand-the-data",
    "href": "05-data-landscape.html#understand-the-data",
    "title": "4  The unique health data landscape",
    "section": "4.2 Understand the data",
    "text": "4.2 Understand the data\nKnowing all the data you’re able to access is critical to understanding what is possible, in addition to understanding the question or problem you are trying to solve.\n\n\n\n\n\n\nTip\n\n\n\nHealthcare data is often complex and ‘dirty’ (inaccurate, incomplete or inconsistent). When possible, liaise with analysts who work within the organisation to gain a local understanding of the data.\n\n\nHealth data is often only available after a significant lag time, and with a slow refresh rate. For example, there is a specific chain of events that lead to updates to national health data collections, and this can take a significant amount of time - often many months.\nAt an early stage, it is valuable to consider:\n\nData landscape - What data is collected? How is access managed? Does it help address the problem you are trying to solve?\nCharacteristics - What is the format, type and size of data? When was the data collected? How often will you receive it?\nAvailability and quality - How much historical data is available, and what is the quality? Use the minimum necessary!\nPurpose - What purpose was the data collected for, and does that influence your interpretation?\nConsent - Is the use of data covered by existing patient consent?\nData collection, maintenance, publication - Distinguish between data already collected and new data created by the study. How do you plan to maintain and/or publish this data?\nPersonally identifiable information - Is de-identification required? Who will do this? (See Handling personal health information)\nData availability - How long will it take to source and are there any reporting or system lags?\nLabelling/annotation - Does the project need Human In the Loop (HITL) mechanism for annotating and validating the input and output data?"
  },
  {
    "objectID": "05-data-landscape.html#sec-ethics",
    "href": "05-data-landscape.html#sec-ethics",
    "title": "4  The unique health data landscape",
    "section": "4.3 Ethics & privacy",
    "text": "4.3 Ethics & privacy\nConcepts around the legal, privacy, and ethical dimensions of health data projects are often interlinked. Legal perspectives consider the fit of the project with the laws of the data source jurisdiction as well as any legal requirements of the analysis location, if these aren’t the same place. Privacy perspectives are around what data is collected, how it’s collected safely, where it is stored, and its lifecycle. Ethical conduct of a project includes ensuring the risks to participants of data use are outweighed by the benefits of the project.\nAll research in New Zealand which uses data from humans needs to be undertaken in an ethical manner. In some cases, approval from a registered ethics committee is required before the project can be started. In many cases, the organisation undertaking the research (such as your employer) may also have specific research or ethical approval processes as well.\nWhen dealing with health data, be careful when assessing if a data science project requires ethical approval. Ethical approval is typically required for any evidence-generating studies, or studies which propose changes to current standard of care (before any model is evaluated/validated) and should always be sought prior to accessing patient data. Ethical approval is given through a written application process and gives permission for the research to be conducted by named investigators during a specific time period.\nEthics approvals in New Zealand are provided through committees which have had centralised approval to follow appropriate standards. The ethical standards are set by the National Ethical Standards for Health and Disability Research and Quality Improvement and apply to researchers, health service providers and disability service providers, regardless of whether or not an additional approval process is required. Think of these like a set of best practices for undertaking work with health data or human participants - follow them regardless of whether anyone is requiring you to do so!\nMost formal approvals for health data research are provided at a national level by the Health and Disability Ethics Committees (HDEC); in some cases a more localised approval is required instead or in addition (for example, the Auckland Health Research Ethics Committee).\nOn top of this, hospitals or clinical organisations usually have their own research offices which require separate notification (such as Research Office, Hauora a Toi Bay of Plenty, Te Whatu Ora).\nFind out if your study requires HDEC review on the HDEC website.\nMany of the questions asked in an ethics application and approval process are important in evaluating the risks and benefits and can indicate if there is social licence for the intended research. This process forces researchers and organisations to consider whether their work has a net benefit for society. The concept of “social license” or societal acceptance for use of health data is a related idea (see Social license - use of health data).\n\n\n\n\n\n\nImportant\n\n\n\nSeek appropriate ethics approvals prior to accessing patient data.\nThe implications of assuming that ethics is not required can have significant downstream effects on a project, such as reputation issues or delays. HDEC can provide ‘Scope of Review’ services to advise if the research you are doing is considered exempt from requiring an ethics application. Consider seeking written evidence of this as an assurance for stakeholders. See the HDEC website for the current process.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nSome ethics applications can take months before an approval is granted, so allow adequate time for this process, factoring in when review meetings are held. It’s also likely that further questions may be asked at this review stage.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nConsider if existing patient consent is sufficient to cover use of the data. See Consent below (reference).\n\n\nGenerally all use of administrative health data for research purposes will need to go through the HDEC process. Consent for use of administrative data is mainly around its use for improving the care of that individual within the health service or quality improvement processes (defined as audits or other activities). Unless the work is specifically with business intelligence within a healthcare organisation, you should ask for assurance to confirm if ethical approval from HDEC is required, or not.\nIn New Zealand, health information follows the Health Information Privacy Code 2020. The rules of this code can be summarised for health agencies as below. These principles are also highly relevant to the use of health data for research purposes.\n\nOnly collect health information if you really need it.\nGet it straight from the people concerned where possible.\nTell them what you’re going to do with it.\nBe considerate when you’re getting it.\nTake care of it once you’ve got it.\nPeople can see their health information if they want to.\nThey can correct it if it’s wrong.\nMake sure health information is correct before you use it.\nGet rid of it when you’re done with it.\nUse it for the purpose you got it.\nOnly disclose it if you have a good reason.\nMake sure that health information sent overseas is adequately protected.\nOnly assign unique identifiers where permitted.\n\nPrivacy Impact Assessments (PIAs) should also be conducted at an early stage to identify potential data protection risks on the data of the individuals included. Measures should be adopted to eliminate or mitigate risks. The Office of the Privacy Commissioner offers some guidance on PIAs.\nUseful resources:\n\nA checklist to address ethical and governance questions\nA Research Ethics Framework for the Clinical Translation of Healthcare Machine Learning\n\n\n4.3.1 Consent\nYou need consent before you can use data - particularly data that contains sensitive information. In the General Data Protection Regulation (GDPR), a regulation in EU law on data protection and privacy, consent is defined as:\n\n“any freely given, specific, informed and unambiguous indication of the data subject’s wishes by which he or she, by a statement or by a clear affirmative action, signifies agreement to the processing of personal data relating to him or her.”\n\n\n\n\n\n\n\nOr use this format for a blockquote?\n\n\n\nSome health providers give patients the opportunity to opt in for their data to be used for research and operational purposes, or to improve their care. Opting in may or may not explicitly allow for the processing of their de-identified data by third parties. More commonly, the patient hasn’t provided explicit consent.\nWhether it’s appropriate to use this data will depend on context (for example public good vs. commercial gain), the degree to which the data is anonymised, whether data is provided in aggregated form, and the purpose the data is being used for.\n\n\n4.3.2 Use and re-use of data\n‘Use’ of data relates to using data for the purpose for which it was consented and collected. Re-use of data (or secondary use) is when you use data collected for another purpose.\nWhen you re-use data, you should be mindful of its purpose, coverage, bias, timeliness, and applicability to the secondary use. For instance, the National Minimum Data Set is gathered for policy formation, performance monitoring, research and review. It may be useful for understanding hospitalisations, but does not provide a complete picture of an individual’s health journey.\n\n\n4.3.3 Social license - use of health data\nSocial licence is the implied permission to make decisions about the management and use of the public data in a way that ensures trust and confidence in the way that data is managed. Organisations who store health data are stewards.\n\n\n\n\n\n\nImportant\n\n\n\nAny use of data, outside of that which has been explicitly consented, is subject to ethics and requires consideration of social licence.\n\n\nConceptually, the level of engagement required to gain social license is higher for decisions that are more likely to affect an individual and are fully automated, compared to those that are manual and affect an entire population. Social licence isn’t ‘gained’ or ‘approved’; organisations need to gauge people’s thoughts, feelings, perceptions on the use of their data. These attitudes are dynamic and constantly evolving.\nAssurance to stakeholders should be provided through transparency and governance to limit reputational harm, particularly when using patient data in a commercial context. Consider what data you’re handling, how identifiable it is and if patient consent is specifically required. This is particularly important for data that may be re-used for secondary purposes - the original consent should be carefully reviewed.\nData should be used for public good in a way that is equitable, with consideration for any unintended consequences, such as increased clinician workload (due to workflow disruption), potential for misuse of the model, and perpetuating biases that exist in the data. Data needs to be treated with care and a suitable data management plan can help.\nIn our experience, privacy, confidentiality, security, transparency, communication and the purpose for which data is used, are often raised as concerns by clinicians, administrators, legal experts, and other stakeholders.\nUseful resources:\n\nA Path to Social Licence Guidelines for Trusted Data Use - Data Futures Partnership 2017 (PDF)\nManatū Hauora research on social licence for health data re-use"
  },
  {
    "objectID": "05-data-landscape.html#sec-sovereignty",
    "href": "05-data-landscape.html#sec-sovereignty",
    "title": "4  The unique health data landscape",
    "section": "4.4 Data sovereignty",
    "text": "4.4 Data sovereignty\nData sovereignty refers to the understanding that data is subject to the laws of the nation within which it is collected and stored. In New Zealand, there is also a focus on where the data is stored and processed. Data agreements should take care to address these points so they are clear to all parties. Data handling and ethical dimensions of the project also need to ensure data sovereignty is at front of mind.\nMāori data sovereignty recognises that Māori data should be subject to Māori governance. Māori data sovereignty supports tribal sovereignty and the realisation of Māori and iwi aspirations. Māori must be included in any work about Māori data. An equity lens is required and ideally should include Māori in the research team as well as in external review/advisor roles.\nUseful resources:\n\nTe Mana Raraunga (Māori Data Sovereignty Network) has helpful resources and guidance.\nThe Health Research Council of NZ (HRC) also produces Guidelines for Researchers on Health Research Involving Māori."
  },
  {
    "objectID": "06-data-access.html#planning-data-access",
    "href": "06-data-access.html#planning-data-access",
    "title": "5  Accessing and managing data",
    "section": "5.1 Planning data access",
    "text": "5.1 Planning data access\nTo understand data access requirements we must consider the sensitive nature of health data, consent, security and technical needs.\nKnow what you are asking for. Your data request should be well specified and considered and it will take some time, discussion with the health data provider and analysis to formulate.\nMany providers will have strict data transfer and storage protocols, however these may not always be followed by individuals. How the data will be transferred and stored, and at what frequency should be discussed and agreed, before a .csv file suddenly lands in your inbox.\nData that contains Protected Health Information (PHI) or Personal Identifiable Information (PII) cannot be freely shared, however after de-identification (see section Data Identifiability) it may be possible to share such data, under certain terms."
  },
  {
    "objectID": "06-data-access.html#data-sharing-agreements",
    "href": "06-data-access.html#data-sharing-agreements",
    "title": "5  Accessing and managing data",
    "section": "5.2 Data sharing agreements",
    "text": "5.2 Data sharing agreements\n\n\n\n\n\n\nTip\n\n\n\nData sharing agreements should be written and agreed early, ideally guided by legal advice.\n\n\nData sharing agreements between a data provider and data recipient typically cover:\n\nWhat data will be shared and for how long\nWhat is the consented use of the data\nHow the data will be shared and stored\nHow and when the data will be destroyed after project completion\nPrivacy and confidentiality\nHow security will be ensured\nHow compliance with and breaches of the agreement will be handled\nWhich parties are responsible for each activity, such as de-identification of data.\n\nThese agreements cannot escape data protection and privacy laws, and agreements that already exist between a data provider and a patient. This should be taken into consideration before any data is transferred."
  },
  {
    "objectID": "06-data-access.html#the-data-request",
    "href": "06-data-access.html#the-data-request",
    "title": "5  Accessing and managing data",
    "section": "5.3 The data request",
    "text": "5.3 The data request\nAnalysis and discussion regarding the data request are also vital at an early stage. Health data is sensitive and describes people in their most personal and vulnerable moments. It is a privilege to be working with it, so be specific with your request and don’t seek more than is really needed.\nSome questions to consider are:\n\nWhat data sources are available and accessible?\nWhat fields are needed?\nCan the data be operationalised?\nWhat time period is to be covered?\nHow do privacy, ethics and equity considerations affect my data request?\nWhat is the minimum data set that I will need for this project?\nWho has the ability to provide the data?"
  },
  {
    "objectID": "06-data-access.html#waiting.and-more-waiting",
    "href": "06-data-access.html#waiting.and-more-waiting",
    "title": "5  Accessing and managing data",
    "section": "5.4 Waiting….and more waiting",
    "text": "5.4 Waiting….and more waiting\nOnce you have your plan in place, smooth sailing isn’t guaranteed. De-identification, data sharing agreements and data requests can take time. But even when an agreement is signed and plans are ready, you might find a lot more time passes while waiting for data.\nHealth data providers usually have to undertake a lot of work before sending data in the form agreed. Data may be located in different systems, extraction may require booking in technical resources and the data may need pre-processing and annotation. Also, while you may be eagerly awaiting the data, it may not be afforded the same priority by a busy health provider. Patience is a virtue, but the occasional gentle nudge may be necessary. You also need to manage your own workload, and you don’t want all of your data buses arriving at once."
  },
  {
    "objectID": "06-data-access.html#transferring-data",
    "href": "06-data-access.html#transferring-data",
    "title": "5  Accessing and managing data",
    "section": "5.5 Transferring data",
    "text": "5.5 Transferring data\nSome providers may require that you use their systems, whether on premises or in the cloud, so that the data never ‘leaves’ their organisation. Others may be happy to upload their data to a secure cloud-based server provided by you, and some may have their own file transfer systems.\nTry to avoid email for data transfer, even if a file is password protected. Emailed data can be difficult to track and audit. This may result in multiple copies being saved using up storage capacity. Files are also frequently too large to be attached. Security is a particular concern, especially where either sender or recipient uses a third-party email service – you do not know how many servers are handling that message.\nIf you will be receiving data regularly, it’s also better to minimise the number of transfers by having data sent in batches. Any data you receive should also be different from what you have already received. Have this conversation with the health data provider at the planning stage.\nFinally, please ensure you confirm successful receipt of the data."
  },
  {
    "objectID": "06-data-access.html#data-governance-and-data-management",
    "href": "06-data-access.html#data-governance-and-data-management",
    "title": "5  Accessing and managing data",
    "section": "5.6 Data governance and Data management",
    "text": "5.6 Data governance and Data management\nData governance and data management are distinct concepts with some areas of overlap and linkage. Both areas concern how we access, store, log, secure, maintain, use, share and destroy data in an efficient and standardised way, guided by principles such as accountability, transparency, ethical use, privacy, consent and data availability, integrity and quality.\n\n\n\n\n\n\n\nData governance\nData management\n\n\n\n\nMaintains oversight, provides accountability, shapes and communicates policies and procedures\nEnacts policies and procedures for day-to-day handling of data\n\n\nGovernance group or executive team or board; a diverse group of stakeholders to cover multiple perspectives\nIndividual data stewards\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis section is written from the perspective of a consulting practice, research organisation or any organisation that uses data which it does not directly collect.\n\n\n\n5.6.1 Data Governance\nFor more information about Governance see the Governance section.\n\n\n5.6.2 Data Management\n\n5.6.2.1 Storage and security\nWith a data access plan in place, and an understanding of what is available, appropriate storage and security requirements need to be in place.\nWhen storing data on premises or on your secure cloud-based service, there should be a clear process for access control. Larger, regular batches of data are preferable, but where there is irregularity in timing and peaks in volume, cloud based auto-scaled storage may help keep costs down.\nOn-premises data storage should be on a secure server, and not stored or replicated on personal computers.\n\n\n5.6.2.2 Data register or log\nWhat starts out as a data trickle can quickly become a data flood. It is worthwhile to maintain a register of data from the first file received.\nA data register or log can help you or your organisation track:\n\nwhat data you have received\nwhen you received it\nwhere you are storing it\nwho can/should have access to it\nwhen it should be destroyed\nwhat processed data sets exist and where, particularly if they contain PHI\n\nThe format can be tailored to suit your organisation (for example, using a spreadsheet).\n\n\n5.6.2.3 Tools for data management\nManaging data for a small organisation or research team could be as simple as having a secure storage solution and a spreadsheet to keep track of important metadata. Larger organisations will likely need formal data governance, data policies in place and data stewards to manage the use, storage and security of data. Tailor your approach based on the data being handled.\nConsider version control for all aspects of the project, including data; depending on the tooling that you use, this may already be an included capability. DVC is one tool designed specifically for data version control for machine learning. Your development environment or cloud storage solution may also include features of this type. Data files can be named with versions, however this method is not as robust as automated versioning that is managed by a tool.\nData management tools are responsible for carrying a wide range of data or documents."
  },
  {
    "objectID": "07-data-identifiability.html#what-information-is-sensitive-pii-and-phi",
    "href": "07-data-identifiability.html#what-information-is-sensitive-pii-and-phi",
    "title": "6  Data identifiability",
    "section": "6.1 What information is sensitive? (PII and PHI)",
    "text": "6.1 What information is sensitive? (PII and PHI)\nDifferent jurisdictions define sensitive data differently. The US federal law Health Insurance Portability and Accountability Act of 1996 (HIPAA) required the creation of national standards to protect sensitive patient health information from being disclosed without the patient’s consent or knowledge. The definitions provided by HIPAA are important to consider even when you are not working in the United States or with data from the USA.\nThe HIPAA defines PII and PHI as:\n\n\n\nPII - Personally identifiable information\nPHI - Protected Health Information\n\n\n\n\nAny piece of information that can be traced to an individual’s identity, not necessarily health related (e.g. address).\n​​Any piece of information in an individual’s medical record that was created, used, or disclosed during the course of diagnosis or treatment that can be used to personally identify them. HIPAA has a detailed definition of PHI.\n\n\n\n\n\n\n\n\n\nLook out for these fields, which HIPAA considers as identifiers\n\n\n\n\n​​Name\nAddress (including subdivisions smaller than state such as street address, city, county, or zip code)\nAny dates (except years) that are directly related to an individual, including birthday, date of admission or discharge, date of death, or the exact age of individuals older than 89\nTelephone number\nFax number\nEmail address\nSocial Security number (in the USA)\nMedical record number or NZ National Health Index (NHI) number\nHealth plan beneficiary number\nAccount number\nCertificate/license number\nVehicle identifiers, serial numbers, or license plate numbers\nDevice identifiers or serial numbers\nWeb URLs\nIP address\nBiometric identifiers such as fingerprints or voice prints\nFull-face photos\nAny other unique identifying numbers, characteristics, or codes\n\n\n\nNew Zealand’s National Ethical Standards provide a shorter list of direct and indirect identifiers:\n\nIdentifier types from the HISO 10064:2017 Health Information Governance Guidelines via the National Ethical Standards\n\n\nDirect identifiers\nIndirect identifiers\n\n\n\n\nNHI\nDate of birth\n\n\nName\nIdentification of relatives\n\n\nStreet address\nIdentification of employers\n\n\nPhone number\nClinical notes\n\n\nOnline identity (e.g., email, Twitter name)\nAny other direct or indirect identifiers that carry significant risk of re-identification\n\n\nIdentification numbers (e.g., community services card, driver’s licence)\n\n\n\n\nYour organisation may have a different name or definition for this information. However it is named or defined, it’s important to note that health data contains sensitive information that patients would not want disclosed. This data is a privilege to use and should be treated with utmost care."
  },
  {
    "objectID": "07-data-identifiability.html#how-do-i-deal-with-sensitive-data",
    "href": "07-data-identifiability.html#how-do-i-deal-with-sensitive-data",
    "title": "6  Data identifiability",
    "section": "6.2 How do I deal with sensitive data?",
    "text": "6.2 How do I deal with sensitive data?\nFrom the regulatory standpoint: the HIPAA has a “Privacy Rule” which requires safeguards when working with protected health information (PHI). Two methods of handling sensitive information which satisfy the HIPAA Privacy Rule are Expert Determination and Safe Harbor.\nWhen a data provider transfers data to you, first check whether you should be in receipt of that data. This requires you to have processes to check the data for PII and PHI. You should also have procedures in place for what to do should you find any PII or PHI, including the destruction of the data, and reporting of the incident.\n\n\n\n\n\n\nImportant\n\n\n\nIf a health data provider has provided you access to PHI or PII containing data in contravention of a data sharing agreement, continued access to or retention of that data is not defensible by reasoning that the mistake was theirs. A guiding principle is that appropriate storage, transfer and use of data is the responsibility of all parties involved.\n\n\nCreating synthetic data is another method for research (see Use of Synthetic Data)."
  },
  {
    "objectID": "07-data-identifiability.html#checking-your-data-for-the-presence-of-identifiable-information",
    "href": "07-data-identifiability.html#checking-your-data-for-the-presence-of-identifiable-information",
    "title": "6  Data identifiability",
    "section": "6.3 Checking your data for the presence of identifiable information",
    "text": "6.3 Checking your data for the presence of identifiable information\nIn general, you are most likely to be working with de-identified data. The data provider is responsible for ensuring that data released is compliant and the data receiver also has a responsibility for highlighting if this has not been done.\nIt is imperative to check that any dataset you receive meets the de-identification standard you are expecting; e.g. you have not been sent a dataset that contains identifiers when it should not.\n\n\n\n\n\n\nTip\n\n\n\nAs a minimum, do a common sense check of metadata and manually scan the first 1000 rows of a dataset for any PHI e.g. unique identifiers, address, and name."
  },
  {
    "objectID": "07-data-identifiability.html#de-identifying-structured-data",
    "href": "07-data-identifiability.html#de-identifying-structured-data",
    "title": "6  Data identifiability",
    "section": "6.4 De-identifying structured data",
    "text": "6.4 De-identifying structured data\nDe-identification can be achieved through the suppression or transformation of certain identifying attributes. For example, a health data provider can be asked to exclude name fields, provide age range rather than date of birth, transform addresses to a statistical area unit, and/or encrypt unique identifiers prior to data transfer.\nEven with suppression or transformation of individual identifiers, individuals can still be identified in structured data where information about an individual is already known. For example, you know someone who is 57, has had breast cancer and lives in a certain postcode. If only one person in a dataset has these attributes, this information could be used to identify that person in the de-identified data. Particularly when linked to other data sources, more personal information could then be gleaned for that person.\nk-anonymisation or ε-differential privacy techniques can be applied to ensure that individuals can’t be identified via combinations of attributes. Both techniques involve a trade-off of privacy and data utility.\n\nk-anonymisation is where at least ‘k’ individuals share an identifying set of attributes for any individual. It can be achieved through suppression of attributes or the generalisation of values (for instance, using age ranges).\nε-differential privacy involves adding noise to the original distribution in a way that ensures that the probability that a statistical query will produce a given result is nearly the same on a dataset that has had one person’s information removed. The higher the level of de-identification, the more noise is added to the distribution."
  },
  {
    "objectID": "07-data-identifiability.html#de-identifying-unstructured-data",
    "href": "07-data-identifiability.html#de-identifying-unstructured-data",
    "title": "6  Data identifiability",
    "section": "6.5 De-identifying unstructured data",
    "text": "6.5 De-identifying unstructured data\nUnstructured data (which is data that doesn’t have a predefined format) poses a particular challenge for anonymisation. It’s estimated that approximately 80% of health data is stored as unstructured text, which is a form of unstructured data.\nBasic pattern matching using regular expressions may go some way to locating identifying text and can be useful for finding attributes with known formats (for example NHI or date of birth). However, free text, even in very short phrases, can contain a sometimes surprising amount of identifying information. Natural language processing techniques and machine learning can be applied for more sophisticated textual de-identification, with the support of de-identification tools."
  },
  {
    "objectID": "07-data-identifiability.html#tools",
    "href": "07-data-identifiability.html#tools",
    "title": "6  Data identifiability",
    "section": "6.6 Tools",
    "text": "6.6 Tools\nSoftware tools which can be of help with the processes of identifying sensitive data and removing it include:\n\nDe-identifier (Orion Health): can identify and remove PHI and PII from datasets and databases to a customisable level of de-identifiability/reidentification risk\nMacie (Amazon): able to check data stored in Amazon Web Services (AWS)\nComprehend Medical (Amazon): provides a PHI detection API\nDLP (Google): able to check data stored in Google Cloud"
  },
  {
    "objectID": "08-data-preparation.html#data-wrangling",
    "href": "08-data-preparation.html#data-wrangling",
    "title": "7  Data preparation",
    "section": "7.1 Data wrangling",
    "text": "7.1 Data wrangling\nData wrangling is the process of cleaning and unifying complex datasets for easy access and analysis. Good data wrangling practice is based on comprehensive knowledge of the data. We recommend the following:\n\nFor a given data source, learn as much as you can about its collection, storage, how it’s updated and maintained, the definition and dependency of each data item, and the limitations. There are more upfront ethical considerations when dealing with health data including consent, ethics approval and understanding the equity and clinical context.\nFor local data collection, consider how to store it (in files, in a database, locally or on cloud etc.) to make information retrieval easier, or/and data sharing easier (see Data Management).\nConsider the size of the data - should it be processed all at once, batch processed or stream processed? Stream processing is the processing of records as a stream of data, for example record by record. Your computing location (on your computer, on a local server, or on a cloud provider) may affect your decision here.\nConsider if the data needs de-identification (see Data identifiability).\nMaintain an equity lens for any type of data science work, including building and evaluating models (see Bias in data).\nInvolve clinical leads when data wrangling. Within healthcare, the risk associated with some data (for example, laboratory data or vitals) is often distributed at the extreme ends. It’s difficult for data scientists to know if the risk is distributed linearly without the clinical context.\n\n\n7.1.1 Data labelling\nData labelling can have a significant impact on the success of a data science project. A label is a category you might tag a record with for a model to learn. For instance, you might label an ED admission according to whether that patient is high priority for triaging, or you might label a medical alert as containing an adverse drug reaction. When learning, a model finds patterns in the data attributes that map to the labels. In the triage example, attributes that are mapped to the label “is high priority” might include body temperature, blood pressure, heart rate and oximeter reading.\nAim for labels that are clearly and appropriately defined, involving multiple qualified labellers.\n\n7.1.1.1 Defining labels\nBe precise, and aim to align the definitions with potential use cases. For example, radiology reports might already be labelled according to the nature of the report findings, such as there was a “finding”, or there was a “finding that is not of concern” or there was “no finding” etc. However, if our goal is to know whether a report requires follow-up and we would like to use machine learning techniques over those reports and labels to achieve this, the initial labels may not be useful: some reports that have a “finding that is not of concern” may need follow-up and some may not. The labels “follow-up required”, or “no follow-up required” are more directly aligned to the use case here.\nBe clear. A clear label definition will help both human labellers and model performance. If the boundary between labels is fuzzy, a person will either struggle to choose a label for a data record, or will apply labels inconsistently. If labels are inconsistent in the training set, then we might reasonably expect that the model will also struggle to find the boundary between labels. In the radiology report example, imagine that the use case aligns with categorising reports according to findings. Consider one report showing that a person has a pre-existing mass considered benign, who then has a follow up scan that reveals the mass has not changed. Different human labellers would need clear definitions to be able to consistently label this report. Without a clear definition, one human labeller may decide that there is a “finding that is not of concern” based on the presence of a mass of no concern, while another may decide that there is “no finding” given nothing has changed since the last scan.\nWhen there is a period of time between labelling iterations, the same human labeller may make inconsistent labelling choices. Clear label definitions will also help with label consistency over time.\n\n\n7.1.1.2 Many labellers make models work\nEven with clear definitions, it is normal to find that different human labellers may vary in how they apply a labelling strategy with no one person being the “source of truth”. Inter-labeller variation should be expected. To reduce the effects of this variation, involve many labellers so that each record has labels assigned by two or more people. Labellers will disagree, so when training a model, you could either decide to use records with labels that human labellers unanimously agree on, or those that the majority of labellers agree on, excluding the records with label disagreement from the training set.\nThe reality is that data labelling in a health context often requires highly skilled and experienced practitioners working under time constraints. If you only have one person who can provide the time required (for what can be a tedious task), then investment in clearly defining labels upfront is especially important.\nFor ongoing labelling efforts, including labelling in a clinical workflow may be an efficient strategy.\n\n\n\n7.1.2 Data quality\nIt’s important to conduct a data quality review as an early step and produce statistical summaries for checking purposes. This process usually includes profiling the fields within a data set, visualising distributions, checking relationships between fields and exploring data completeness (including completeness over time). Involve the data owners and subject matter experts in this process and consider additional rounds of quality reviews if the data is complex.\nData profiling (for example, with Pandas-profiling) is recommended for both data quality checking and data understanding. Data profiling involves providing descriptive summaries and visualisations of the data for review with clinicians and subject matter experts.\nBasic aspects for data quality check include:\n\nData availability across time\nData distribution change across time\nOutliers: Do we need to cap variables with extreme values?\nData completeness/missingness\nDuplication in the data\nTarget label distribution (data imbalance)\nAssociation between features and the target\nData timeliness: The data resource provides the data in appropriate time.\nScalability: The data can be accessed from many components without losing its meaning\nProvenance: The data should be based on valid authority\nLocality: The location of data resource should be provided to check relevance of data\nStructure: form of data\nAdoption: The data are useful for the project purpose and adaptable with the requirements, including operational requirements (will the data be refreshed, timely and in the expected format?)\nIdentification of extreme/outlier values, and variables that require capping."
  },
  {
    "objectID": "08-data-preparation.html#data-linkage",
    "href": "08-data-preparation.html#data-linkage",
    "title": "7  Data preparation",
    "section": "7.2 Data linkage",
    "text": "7.2 Data linkage\nMany projects that involve health data will require linking datasets on certain attributes (keys). For example we might link datasets by matching records that share the same NHI, by matching records that share a combination of personal details, or by matching locations.\nOne example is linking data across government departments or agencies, for instance linking hospitalisation data to vaccination data, matching on NHI. Another example is where individual health data is linked to census data based on census meshblock.\nData linkage refers to this joining of datasets. It allows us to get a more detailed picture of a person or entity. For instance, when linking medical records to census records, this might reveal that a patient resides in an area of high deprivation.\n\n7.2.1 Unique identifiers\nNHI is a unique health identifier that is commonly used to link datasets in health. As a unique identifier, each NHI should identify a single person and each person should have a single NHI. However, in certain cases multiple NHIs get recorded for a single person. We note that records with a single unique identifier, and those with multiple “unique” identifiers, can have equity considerations in health and indicate the nature of interaction with the health system.\n\n\n7.2.2 How to link\nIdeally datasets would be linked prior to de-identification, so that the linkage can be as robust as possible. Further, de-identification after linking can help minimise the risk of re-identification via linked data (see De-identifying structured data).\nWhere datasets need to be de-identified prior to linking at the level of the individual or patient, you will need to consider how to maintain the integrity of the linkage. Unique identifiers, such as NHI, may need to be encrypted, rather than suppressed, in the process of de-identification.\nWhere data comes from different sources, unique identifiers will need to be encrypted in the same way in order to link datasets. If the processed data is to be delivered back to the health provider at the level of the individual, then there needs to be the possibility of decryption of the identifiers or another way of linking the processed data back.\n\n\n7.2.3 Risk of re-identification\nTwo datasets that separately will not identify an individual may identify an individual once linked. As a result, the risk of re-identification of an individual should be considered prior to linking datasets. Ideally, datasets are linked together and de-identified prior to delivery from the health provider; however, this is often not possible.\n\n\n7.2.4 Approaches\nWhere data cannot be linked on a unique identifier, there are approaches that can be used to match records. The ‘deterministic’ approach is to match on a combination of attributes that will uniquely identify a person. An issue with this approach is that in a dataset that is de-identified well, it should be difficult to uniquely identify individuals based on a combination of unique attributes.\nThe alternative ‘probabilistic’ approach involves calculating conditional probabilities to determine the likelihood that a given pair of records match."
  },
  {
    "objectID": "08-data-preparation.html#missing-data",
    "href": "08-data-preparation.html#missing-data",
    "title": "7  Data preparation",
    "section": "7.3 Missing data",
    "text": "7.3 Missing data\nMissing data without context should be interpreted carefully. For example, missing hospitalisation records might indicate different access to care or it could mean the data was not recorded; you may not have access, or you’re healthy and haven’t needed to see your GP. Care must be taken in interpreting missing data, as it can be hard to be sure of the context around the missing data. This illustrates why clinical engagement, and stepping back to see the wider picture, is vital in health data science projects.\nMissing data is a common problem with most health data sets. When you encounter missing data, you can choose how to manage it, by either:\n\nRemoving it\nInterpreting it as “not applicable”\nImputing it (fill it in with other values).\n\nOften a combination of removing excessively missing (according to a pre-determined threshold) observations and variables, and then imputing the remaining missing values, is effective.\nWhich option you choose should be based on whether your method is tolerant of missing values; whether the data appears to be ‘missing completely at random’ (MCAR) in which case it could be ignored; or whether there are some patterns in its missingness.\nIf you choose to remove the data, you’ll need to decide whether to remove variables or observations (for example, rows or columns).\nTry adding ‘proxy’ variables or altering data collection processes to avoid missing data as much as possible.\nMost data is unlikely to be MCAR. Often, data is more likely to be missing for particular reasons (defined as ‘missing not at random’ (MNAR)). Consider if there’s a reason for the missingness, as this can itself be informative. For example, data may be missing from smaller subgroups within the dataset. This is particularly important when considering stratification for ethnicity.\nIt’s important to consider the potential bias introduced into a model by removing missing data, or the impact on equity if data is removed. Before continuing with imputation or removal, check for patterns in the data of individuals who have partial missing data to assess the equity and bias implications of removing or imputing it. Look for evidence of non-random missingness by comparing the complete and missing data groups through stratification for important demographic variables such as age and ethnicity.\nImputation can be a useful technique for overcoming missing data problems, but can be computationally intensive. Through a PDH research project, a guide on multiple imputation was published with information about the application of imputation techniques. Your strategy for handling missing values will have implications for how you handle future unseen data too."
  },
  {
    "objectID": "08-data-preparation.html#use-of-synthetic-data",
    "href": "08-data-preparation.html#use-of-synthetic-data",
    "title": "7  Data preparation",
    "section": "7.4 Use of synthetic data",
    "text": "7.4 Use of synthetic data\nAccessing real-world data can sometimes be difficult, whether due to timing, privacy concerns, access problems, lack of participation in healthcare by certain groups, or rarity of a disease.\nSynthetic data (information that’s artificially generated rather than produced by real-world events) can be used if you’re having difficulty accessing real-world datasets. Synthetic data may also satisfy a use case (for example, for data augmentation purposes).\nSynthetic data can be used for prototyping or building analytics dashboards that are ready to plug into real-world datasets. It can also be used for machine learning purposes, including to integrate data relating to under-represented conditions and groups of people, and to protect privacy. Recent publications have overviewed the use of synthetic data (see Chen et al. 2021).\n\nChen, R J, Lu, M Y, Chen, T Y, Williamson, D F K, and Mahmood, F. 2021. “Synthetic data in machine learning for medicine and healthcare”. Nature Biomedical Engineering 5.6, 493–497. https://www.nature.com/articles/s41551-021-00751-8.\nHowever, off-the-shelf models that generate synthetic data (such as Synthea) are not mature and should be carefully assessed for local use. We highly recommend that you’re aware of how the synthetic data is generated and what the limitations are before using it.\nOff-the-shelf calculators can also be considered in lieu of building or enhancing models with synthetic data, with a caveat that these calculators are ideally subject to local validation. There are tools available such as Te Pokapū Hātepe o Aotearoa New Zealand Algorithm Hub which provide a shared knowledge-base of pre-trained models, algorithms and risk calculators, reviewed by a multidisciplinary governance group."
  },
  {
    "objectID": "09-modelling.html#modelling-process",
    "href": "09-modelling.html#modelling-process",
    "title": "8  Modelling",
    "section": "8.1 Modelling process",
    "text": "8.1 Modelling process\nAlthough the details can be different case by case, there are several general steps and a few principles that may help to achieve good results.\nModelling process overview Adapted from W. Hare, A. R. Rutherford, K. Vasarhelyi and The Complex Systems Modelling Group: Modelling in Healthcare, American Mathematical Society (2010)\nScoping should clearly define what problem a model is intended to solve. This affects almost all the later steps. In general, a more specific scope is more likely to lead to success, which should also be defined at the outset. Documenting the scope usually involves sign-off from oversight groups, and can reduce wasted effort or misunderstanding."
  },
  {
    "objectID": "09-modelling.html#modelling-approaches",
    "href": "09-modelling.html#modelling-approaches",
    "title": "8  Modelling",
    "section": "8.2 Modelling approaches",
    "text": "8.2 Modelling approaches\nOne general principle of modelling design is to select models that are as simple as possible while capable of providing solutions to the defined problem.\nThere are often multiple modelling approaches suitable for solving one problem, and each has advantages and disadvantages. If capacity and feasibility allow, it is often beneficial to utilise more than one technique - similar insights provided via different approaches add to the credibility of modelling.\nThe choice of technique is influenced by your ability to be able to explain how it works to end users and operationalise the inputs to the model. Consider this prior to choosing which model is used.\nA non-exhaustive list of categories of quantitative models and the example healthcare problems that they are used for:\n\n\n\n\n\n\n\nModel\nExample use case\n\n\n\n\nDescriptive statistics\nUnderstand disease prevalence and demographics of patient cohorts\n\n\nRegression model\nRelationship between risk factors vs. cost of cancer treatment\n\n\nClassification model\nIdentify high risk patients\n\n\nClustering model\nIdentify different patient cohorts\n\n\nSimulations\nWorkforce planning during a pandemic using simulated disease spread trend\n\n\nOptimisation\nMinimising surgeons’ overtime while planning as effectively as possible to meet surgery demands\n\n\nSystem dynamics\nGP training plan for the next ten years given the population and workforce dynamics\n\n\nLanguage models\nModel-aided auto coding of clinical documents\n\n\n\nFor more models in healthcare, please refer to W. Hare, A. R. Rutherford, K. Vasarhelyi and The Complex Systems Modelling Group: Modelling in Healthcare, American Mathematical Society (2010)."
  },
  {
    "objectID": "09-modelling.html#documentation-and-management",
    "href": "09-modelling.html#documentation-and-management",
    "title": "8  Modelling",
    "section": "8.3 Documentation and management",
    "text": "8.3 Documentation and management\nDocumenting all the steps and experiments is highly recommended. Specific information about the software used in model development and the modelling environment itself should be comprehensively documented, including the language or software used, dependencies, and version numbers, to enable the model to be able to be reproduced. Frequently a package is updated and causes a change that affects the production, performance, or other aspects of a model.\nFor machine learning model management, there are open-source tools such as MLflow to efficiently manage the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. Tools including pipenv/renv assist portability and reproducibility, and help generate this documentation.\nProviding an explicit statement of model assumptions to analysts or end users e.g. input parameter values chosen, uncertainty in those values, deterministic vs stochastic modelling, sensitivity analyses.\nUse of a version control system such as git is very important for coordinating collaboration, avoiding rework, auditing, and generally maintaining your own sanity."
  },
  {
    "objectID": "09-modelling.html#modelling-tools",
    "href": "09-modelling.html#modelling-tools",
    "title": "8  Modelling",
    "section": "8.4 Modelling tools",
    "text": "8.4 Modelling tools\nCommonly used tools for statistical analysis and modelling that require minimum programming:\n\nStatistical Package for the Social Sciences (SPSS). Suitable for descriptive statistics, parametric and non-parametric analyses, and visualisation of analysis results.\nMicrosoft Excel. Easy to have some initial observation over tabular data, suitable to generate summary metrics of data, and create simple data visualisations. Excel has some notable drawbacks including:\nA limitation for showing up to 1,048,576 rows by 16,384 columns in a worksheet\nAs an artefact, it is difficult to audit and rerun with different data\nThe data and analytics layers are entangled, which can compromise data integrity.\nCloud based modelling platforms such as BigML (https://bigml.com/). These platforms provide a web interface for users to easily run machine learning modelling experiments as well as interpretability and collaborative features. However, users may need to upload their data to the cloud, which may require additional data policy checking for the use case.\n\nCommonly used programming tools for statistical analysis and modelling:\n\nR. R is a fully open-source software environment for statistical computing and graphics. R has a GUI Rstudio, where R markdown can be a plug-in to facilitate R code and markdown based report/documentation generation and thus collaboration. The R community is active in developing and maintaining various analysing and modelling packages. Essential packages for data science includes:\n\nFor data manipulation: dplyr, tidyr, dbplyr\nFor modelling: caret, e1071, mlr3\nFor visualisation: ggplot2, plotly\nFor report generation: knitr\nFor creating interactive web interface: shiny\n\nPython. Python is a high-level object-oriented programming language with good code readability. Python has a comprehensive ecosystem to support common statistical analysis, machine learning, deep learning and the implementation and deployment of these data science features. Jupyter notebook/lab is a widely used tool in the Python ecosystem for shareable data science work. Many dependencies in the Python ecosystem are open-source. Essential python dependencies for data science include:\n\nFor data manipulation: pandas, numpy\nFor statistical analysis: scipy, statsmodels\nFor modelling including machine learning: scikit-learn, statsmodels\nFor deep learning: tensorflow, pytorch, keras\nFor visualisation: matplotlib, seaborn, plotly, bokeh\nFor creating interactive web interface: dash, streamlit\n\nSAS. SAS is a statistical software suite for data management, advanced analytics, multivariate analysis, predictive analytics and so on. SAS has its own GUI for non-technical users but programming via the SAS language can provide more flexibility and functionality in the analysis. It provides free SAS Ondemand for Academics (previous University Edition) for non commercial users such as students and educators.\nMatlab. Matlab is a programming language and environment optimised for numeric computing. Especially for matrix manipulation, Matlab has advantages compared with the aforementioned other tools. For machine learning and deep learning, Matlab provides the Statistics and Machine Learning Toolbox and the Deep Learning Toolbox, respectively.\n\nOpen source tools such as R and python provide transparency, collaboration and a low barrier to entry. Commercial tools tend to have a more sophisticated user experience, but a limited ecosystem, and provide some assurances (e.g. extension/library support) that aren’t available through commercial partners."
  },
  {
    "objectID": "09-modelling.html#experiment-tracking-tools",
    "href": "09-modelling.html#experiment-tracking-tools",
    "title": "8  Modelling",
    "section": "8.5 Experiment tracking tools",
    "text": "8.5 Experiment tracking tools\nThe process of developing a model is iterative, potentially involving trial and error for experiment configuration tuning, training and evaluation of the model. When the complexity of a model grows, monitoring experiment configurations and model performance could become a hassle. Using automatic experiment tracking tools such as Tensorboard, weights & biases are helpful to overcome those issues. In general, experiment tracking tools provide features such as visualisation, logging, integration with multiple ML frameworks and model profiling. Also, such tools provide integration facilities for CI/CD pipelines, ML lifecycle management tools which helps to streamline the model training and deployment pipeline while keeping traces in the experiment lifecycle.\nThere are lists online which suggest experiment tracking tools for machine learning note an example list."
  },
  {
    "objectID": "09-modelling.html#model-validation",
    "href": "09-modelling.html#model-validation",
    "title": "8  Modelling",
    "section": "8.6 Model validation",
    "text": "8.6 Model validation\n\n8.6.1 Data partitioning\nModels are usually tuned to the dataset that they are trained with. Using independent datasets for validating the performance of a model is preferred. However, in the healthcare domain, data sharing can make this difficult.\nWhere independent datasets are not available, the model needs to be validated using the development dataset. There are two main approaches for train-test data splitting and model validation.[^1]\n\nCross validation\n\nThis method mixes and shuffles all the data points and splits them into k folds (usually 5-10) for iterative performance measuring. In each iteration, one fold is left out as the testing set and the others are used as the training set. By these iterations, k measures of the chosen performance metric(s) could be obtained to assess the model’s average performance and its variance, which is used as an estimate of the model’s performance on unseen data.\n\n\n\nExample of K-Fold Cross Validation when k=5\n\n\nTo use this method, it is important to make sure that there are no significant data changes along the time within the model development dataset, and no foreseeable data changes between the development set and the future data that the model would be applied on.\nIn healthcare, it is also important to check whether the model will be used at an event level or patient level in advance.\nThere could be multiple events for the same patient in a dataset and if no proper consideration is taken into account while splitting the testing data (or other preprocessing like deduplication), this could potentially lead to information leakage and inaccuracy.\nFor example, in a longitudinal dataset of health encounters, the dataset could be split to group all of a patient’s encounters together (so these are not split across sets), or split by time (which could mean a patient’s encounters are split between different validation sets). Consider whether these types of splitting will make a meaningful difference to your validation exercises. Also be mindful of anything that involves a flow or transfer.\n\nTime-wise validation\n\nMany healthcare model applications have in nature a time dimension, as data are collected by healthcare events that happen across time. There could be a trend in the data as people age, population structure changes, and the healthcare technologies/systems evolve. > To better estimate the future performance of a model, a time-wise splitting and validating approach can be taken. This method sets data of a specified period along time as the testing set and uses the data before the testing set as training data. A visualisation of performance change over time will be obtained after a number of iterations, which provides an estimation of the trend of the model’s performance change in the near future - will it be relatively stable, gradually decreasing or increasing.\nSome models have complex hyperparameter space and require an additional split of data for early stop in training or additional hyperparameter optimisation. In classification models that rely on a cut-off threshold in application (e.g. a high risk patient identification model based on numeric risk scores), the cut-off threshold can also be seen as part of the model hyperparameters, and its determination should be considered as part of the model development. Hence the performance validation process should be using a partition of data that is not used in either model training, additional hyperparameter optimisation, or the threshold optimisation in particular.\nIn all cases, care should be taken to ensure that the sampling unit is complete. For instance, we may be modelling a flow of events, in which case the sampling unit is likely to be a patient. If the modelled data consists of multiple records per patient, it will be important to ensure that complete patient records are sampled.\n\n\n8.6.2 Performance metrics\nThe type of performance metric used will depend on the model being evaluated and the context of the project. For models that predict a value, such as a linear regression model, R-squared and root mean square error (RMSE) are common.\nIn health, metrics calculated from a confusion matrix for models that output a class (e.g. a decision tree) or probability (e.g. logistic regression) are commonly used. To produce a confusion matrix for a model that outputs a probability, a probability threshold is applied (values above the threshold are positive and values below are negative). The probability threshold can be set to optimise a particular metric, can be set by other optimisation techniques, or otherwise set according to the use case. Seek clinical input on the appropriate thresholds as they can impact patient care.\nCommon performance metrics used can include the following. Note that not all are appropriate for each model or data type or question being answered.\n\nPrecision quantifies the proportion of positive class predictions that actually belong to the positive class.\nRecall quantifies the number of positive class predictions made out of all positive examples in the dataset.\nModel accuracy is a machine learning classification model performance metric that is defined as the ratio of true positives and true negatives to all positive and negative observations.\nF-Measure provides a single score that balances both the concerns of precision and recall in one number.\nPositive and negative predictive values (PPV and NPV respectively) are the proportions of positive and negative results in statistics and diagnostic tests that are true positive and true negative results, respectively. The PPV and NPV describe the performance of a diagnostic test or other statistical measure.\n\nPositive predictive value (PPV) and negative predictive value (NPV) are best thought of as the clinical relevance of a test\n\nPrevalence is the number of cases in a defined population at a single point in time and is expressed as a decimal or a percentage.\nSensitivity is the percentage of true positives (e.g. 90% sensitivity = 90% of people who have the target disease will test positive).\nSpecificity is the percentage of true negatives (e.g. 90% specificity = 90% of people who do not have the target disease will test negative).\nAUROC is a performance metric for “discrimination”: it tells you about the model’s ability to discriminate between cases (positive examples) and non-cases (negative examples.) For a ranking use case AUC-ROC is a measure that indicates how good the model is at ranking cases based on a score (how likely any two cases are correctly ordered). 0.5 indicates that the model is no better than random at ranking and 1 indicates a perfect model.\nAUC-PRC is a measure that indicates how good the model is at minimising the tradeoff between precision and recall. A high AUC-PRC represents the model can achieve high recall and high precision at the same time."
  },
  {
    "objectID": "09-modelling.html#further-resources",
    "href": "09-modelling.html#further-resources",
    "title": "8  Modelling",
    "section": "8.7 Further resources",
    "text": "8.7 Further resources\nRules of Machine Learning: Best Practices for ML Engineering (Google Machine Learning Guides)\nErdemir, A., Mulugeta, L., Ku, J.P. et al. Credible practice of modeling and simulation in healthcare: ten rules from a multidisciplinary perspective. J Transl Med 18, 369 (2020). https://doi.org/10.1186/s12967-020-02540-4\nW. Hare, A. R. Rutherford, K. Vasarhelyi and The Complex Systems Modelling Group: Modelling in Healthcare, American Mathematical Society (2010)\nA Practical Guide to Maintaining Machine Learning in Production"
  },
  {
    "objectID": "10-evaluation.html#model-safety-false-negatives-false-positives",
    "href": "10-evaluation.html#model-safety-false-negatives-false-positives",
    "title": "9  Evaluation",
    "section": "9.1 Model safety (false negatives, false positives)",
    "text": "9.1 Model safety (false negatives, false positives)\nFrom a clinical perspective the main concern/clinical risk often concerns false negative/low-risk individuals with a positive outcome. Include clinical experts in the discussions on appropriate model thresholds.\nWhen models are implemented, people with high risk will often have additional things done for them to reduce the risk (less of a concern as we are actively doing something to reduce the risk). For the low-risk individuals, there are two possible explanations:\n\nThe outcome was unpredictable\nAdditional data could be used for prediction\n\nIt is usually difficult for a data scientist to know what additional data could have been used and it is helpful for the clinical lead to audit a number of false negative results to determine if the outcome was truly unpredictable or if additional data would have helped with the prediction."
  },
  {
    "objectID": "10-evaluation.html#bias-in-data",
    "href": "10-evaluation.html#bias-in-data",
    "title": "9  Evaluation",
    "section": "9.2 Bias in data",
    "text": "9.2 Bias in data\nData is collected in a particular context, which leads to bias. This can come from different sources including historical bias, data imbalance, missingness, and human prejudice. There is no single best definition of bias or fairness that applies equally well for every data science application.\nWhile training machine learning models using historically collected data, or drawing any conclusion from data, we should be typically mindful about the potential bias in the data regarding sensitive attributes such as age, ethnicity and gender. Bias-related harms can be reinforced by machine learning models/systems.\nMachine learning fairness itself is a broad topic. For a non-exhaustive summary of machine learning fairness from a technical perspective please refer to ML fairness. (Note Bellamy et al. 2018)\n\nBellamy, R K E, Dey, K, Hind, M, Hoffman, S C, Houde, S, Kannan, K, Lohia, P, Martino, J, Mehta, S, Mojsilovic, A, Nagar, S, Ramamurthy, K N, Richards, J, Saha, D, Sattigeri, P, Singh, M, Varshney, K R, and Zhang, Y. 2018. “AI fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias”. http://arxiv.org/abs/1810.01943.\nDue to the history focus of cohort studies, certain groups of the population, such as certain ethnic groups, females, might be under-represented and more vulnerable to bias in such studies while any conclusions were drawn or models were trained. In evaluation work, it is important to measure the goodness of fit, accuracy and other metrics of a model from multiple perspectives rather than the overall metrics only. Basic measurement aspects with respect to sensitive attributes (e.g. gender, ethnicity) to be considered:\n\nThe difference of actual patient data metrics stratified by sensitive attributes - whether there is any inequity among the stratified groups, and what bias it may bring into the models\nThe difference of predicted outcome metrics stratified by sensitive attributes - whether there is any inequity in model predictions among the stratified groups, and what downstream consequence this may cause\nThe difference of model performance metrics among the stratified groups - whether the model is treating the groups equally, and what downstream consequence this may cause\n\nReporting metrics with stratification by sensitive attributes whenever applicable can help maintain an equity lens more easily. Performance of the IBIS/Tyrer-Cuzick model of breast cancer risk by race and ethnicity in the Women's Health Initiative is an example see Kurian et al. 2021.\n\nKurian, A W, Hughes, E, Simmons, T, Bernhisel, R, Probst, B, Meek, S, Caswell‐Jin, J L, John, E M, Lanchbury, J S, Slavin, T P, Wagner, S, Gutin, A, Rohan, T E, Shadyab, A H, Manson, J E, Lane, D, Chlebowski, R T, and Stefanick, M L. 2021. “Performance of the IBIS/tyrer‐cuzick model of breast cancer risk by race and ethnicity in the women’s health initiative”. Cancer 127.20, 3742–3750. https://onlinelibrary.wiley.com/doi/10.1002/cncr.33767.\nThere is a trade-off between data informativeness / model performance and fairness. Most bias mitigation methods cannot avoid playing with this balance. It is highly recommended to take into account the use case and follow-up impacts while deciding which bias mitigation method to be used and how to use it.\nMitigating bias and improving fairness is mostly not a technical challenge but a much broader systematic challenge.\nWe recommend including diverse voices and perspectives in data science work, e.g., having a Māori researcher(s) in the project.\nEven if no mitigation can be done, it is recommended that the bias itself should be analysed and reported if possible, especially that it is important to identify who is most vulnerable to the bias-related harms.\nTools:\n\nAequitas\nAudit-AI\nAI Fairness 360 (IBM)\nFairlearn (Microsoft)\nThe LinkedIn Fairness Toolkit (LiFT)\nFairness Indicator (Google)\nPROBAST\n\nFurther resources:\n\nFATE: https://www.microsoft.com/en-us/research/theme/fate/#!publications\nhttps://www.thinkwithgoogle.com/feature/ml-fairness-for-marketers/\nML fairness gym\nMinistry of Health - Emerging Health Technology Advice & Guidance"
  },
  {
    "objectID": "10-evaluation.html#clinician-in-the-loop",
    "href": "10-evaluation.html#clinician-in-the-loop",
    "title": "9  Evaluation",
    "section": "9.3 Clinician in the loop",
    "text": "9.3 Clinician in the loop\nexpand"
  },
  {
    "objectID": "10-evaluation.html#sec-transparency",
    "href": "10-evaluation.html#sec-transparency",
    "title": "9  Evaluation",
    "section": "9.4 Transparency, interpretability, and explanation",
    "text": "9.4 Transparency, interpretability, and explanation\nIn the context of health it is especially important to communicate findings in a way that builds trust in the model development process and outputs. If this is not done well, people may not adopt a model in practice that would otherwise have positive health impacts.\nThe requirement that models are transparent, interpretable and explainable may guide early modelling decisions such as which algorithm to use and how to treat inputs. Linear models tend to be more interpretable and explainable, so too are models that are built with inputs that have not been subject to significant transformations or weightings. The choice of a ‘simpler’ model may compromise model performance meaning better outcomes are sacrificed for the sake of explainability, however there is little point in deploying a deep learning model that has excellent performance, but is not trusted or used. This trade-off needs to be carefully considered.\nConsider outcome measures that are tangible and meaningful to the end user, and that have some relationship to the project goal, such as hospitalisation, mortality, or rankings.\nIndicate feature importance and how the model inputs are weighted in relation to model outputs and what that means in practice. For example, if a model includes modifiable inputs, a model user will want to know how changes to that input might affect an outcome. From an equity perspective, a model user will also want to know to what extent inputs such as ethnicity, age, gender or deprivation impact the outcome.\nThese considerations are also relevant to governance of models. GDPR’s regulation specifically emphasises a model’s transparency, accountability and governance (see Kaminski and Malgieri 2021).\n\nKaminski, M E and Malgieri, G. 2021. “Algorithmic impact assessments under the GDPR: Producing multi-layered explanations”. International Data Privacy Law 11.2, 125–144. https://academic.oup.com/idpl/article/11/2/125/6024963.\n\n9.4.1 Transparency around the inputs\nProvide good data definitions and reasons for how data has been treated e.g “The model includes age but it has been grouped into 3 categories (18-39, 40-59 and 60+) to simplify the model and handle outliers without compromising performance”, and, “The count of regular medications includes medications that have been prescribed in the two years prior to test positive date for this infection. Regular medications are medications that have been prescribed at least four times over that period. Prescription data rather than dispensing data is used as it has better coverage.”\nProvide the provenance of data e.g. “This data came from a national collection of data that went through a quality assurance process, it is current and relevant to the problem being answered in this way…”\n\n\n9.4.2 Interpretation of the output\nThere are different types of output from different algorithms, e.g. risk scores, predictions, simulation results. In particular, users need to understand what the value means in the context of how it will be used. For example, a risk score of 0.8 might mean 80% probability of an outcome or it may mean something else depending on how the model is built and the use case. If the risk score has been developed for a ranking use case, the end user will need to understand that the output for a given person has meaning in relation to outputs for other people to determine who is at higher risk, rather than as a standalone value.\n\n\n9.4.3 Transparency of algorithm development\nAuditing the behaviour of an algorithm at the population level. For example, does the relationship between the predicted values and certain covariants (e.g. increased predicted mortality risk vs. age) for the validation cohort match with empirical evidence or the clinician’s cognition? Interpretation techniques such as partial dependence plots can facilitate such inspection.\nProviding individual level prediction reasoning - provide explanations of the prediction for a specific individual. For example, why the algorithm predicted a 0.86 readmission risk score for a 75 years old Pasifika woman. The score can be attributed to her age, previous hospitalisation history, cancer diagnosis, ethnicity and a few other risk factors. Shapley values is one of the commonly adopted techniques to provide explanations at the individual level. Christoph Molnar 2022 provides more details about model interpretation techniques.\n\nChristoph Molnar. 2022. “Interpretable machine learning”. https://christophm.github.io/interpretable-ml-book/index.html.\nClinicians or other stakeholders need to be involved in the population and individual level algorithm auditing, and drive the iterations of the algorithm development with their feedback.\nWhere possible, making the code base and dataset public adds credibility\n\n\n9.4.4 Understanding the performance of an algorithm\nDefine performance metrics in the context of the problem. Plain language and accessible explanations not only help build trust in the model, they help the user understand how to use the model, encouraging adoption.\nTake, for example, a use case where a model is being used to predict a condition (with prevalence 2%) that requires an intervention. A positive predictive value of 0.95 means that of those people that presented with the condition, 95% were identified by the model. Putting this into context we could say that if 1000 people a day were assessed, we would expect that 20 of them would require the intervention. The model would identify 19 of those people, meaning weekly, 5 people with that condition would be missed if we were to rely solely on model outputs.\nInformation like this can help a clinician understand that while the model performs well, in practice they might like to supplement model outputs with other assessments.\nFor models that output probabilities, such as logistic regression, such analyses can be useful in helping clinicians quantify and understand the trade-off between false positives and false negatives in order to decide which decision thresholds may be appropriate.\n\n\n9.4.5 Understanding the impact of an algorithm\nEvaluate model benefit and cost in the context of realistic scenarios. With classic model performance metrics such RMSE, accuracy, precision or recall, it is often not enough to illustrate the consequences of integrating the algorithm into a healthcare workflow. It needs to be understood and documented how the algorithm would be integrated in a workflow, and is worth further evaluating what could be the potential healthcare outcome, especially when there is a clinical capacity limitation. For example, a model is developed to classify GP referred patients into high priority and low priority using a certain priority score threshold, and is targeted to facilitate timely triaging. However, in the system there are many people already waiting in the triage queue, people newly referred each day, and the number of triages the clinicians can process has a limit and uncertainty. By just looking at the classification metrics of this model, without knowing at which step of the process this model will be used and how, it is hard to tell exactly whether the integration of the model will bring more benefit than cost in the waiting time for patients who are in urgent need.\nDefining proper impact metrics according to the use case and goal of modelling, and carefully running through a further evaluation given the workflow will provide more informative insights than just the classic model performance metrics. Deterministic or stochastic simulation techniques can be applied for such evaluation when a working scenario can be quantitatively described. As a straightforward example, the New Zealand business case for hospital avoidance programme using a readmission risk model presented its financial impact in healthcare (Vaithianathan et al. 2012).\n\n\n9.4.6 Principles to follow\n\nClosely engage with the stakeholders and data providers (covered in End-user engagement)\nKeep clinicians in the loop (Clinician in the loop section heading added)\nHave good quality documentation to share the work with others. Transparent Reporting of a multivariable prediction model for Individual Prognosis or Diagnosis (TRIPOD) provides a well established and practical template for healthcare model reporting. For simpler reporting, consider Minimum information about clinical artificial intelligence modeling: the MI-CLAIM checklist as an alternative. For key information from governance perspective, refer to the governance process and Algorithm Information Request template for the New Zealand Algorithm Hub (https://algorithmhub.co.nz/about).\nVersion control the code base so that work is reproducible\nData quality checking: availability including operational concerns, sanity checking, bias\nEquity concerns: who benefits from the algorithm; who may be vulnerable\nConsider your audience when presenting data"
  },
  {
    "objectID": "11-deployment-lifecycle.html#sec-deployment",
    "href": "11-deployment-lifecycle.html#sec-deployment",
    "title": "10  Deployment & lifecycle maintenance",
    "section": "10.1 Operational deployment",
    "text": "10.1 Operational deployment\nHealth data science projects often aim to produce tools that are intended to be used in decision support - through presenting insights or suggestions. Thinking through the implications of the intended use in practice is critical to success.\nWe recommend planning for this from the start if operational deployment is an explicit goal as there is a big difference between research and operationalising something. Engage early with SMEs to gain the right mix of expertise for a successful implementation.\nSome questions worth considering include:\n\nWho are the users, and what knowledge/capability/training will they need?\nIs all the relevant data available at the point it is intended to be used, and is it current?\nIs it feasible to get operational data supply ongoing to support this?\nHow will models/decisions/insights be presented to the user? Can users understand how the model came to a conclusion and what action is required? See section on Transparency and Interpretability\nWhat monitoring, IT security and safeguards are likely to be necessary?\nWhat systems will be utilised, and who is responsible for them?\nWho will provide governance oversight of the model?\nWill the model be updated when new data becomes available?\nWhat exactly is expected to be deployed? An API? A frontend?\nHow are the models and API going to be versioned?\nHow much traffic are you expecting?\nHow much response time and performance in time is acceptable ?\nWhat level of work is required from warehouse developers when developing models? - a simple model is more likely to be implemented than one which is complicated even if there is a performance decrease.\n\nThe below list outlines ideas and recommendations concerning operational deployment of ML solutions within the medical sector:\n\nDraw clear lines between research, analytics and software development. If you are looking to offer software as a service, then you need software engineers who are familiar with software development practices (e.g. writing production-grade code, developing infrastructure) that are necessary to succeed.\nKeep things simple: most prospective customers in the medical sector are conservative and they need to understand (at least to some extent) solutions that are being offered to them and they are also most probably using outdated solutions (as opposed to cutting edge research)\nMany prospective customers in the medical sector are going to be unwilling to send their data to an API over the public internet. You may either need to specifically focus on the niche of customers who do not have these constraints or you need to develop solutions that can be deployed on premise. This has profound implications on your architectural decisions (e.g. federated learning), your ability to provide maintenance and support as well as your ability to keep tight control over your source code\nIdentifiable health data is regulated around the world and you need to ensure that you follow the right security practices for your clients to be able to use your services. This also has downstream implications on debugging, logging, performance tracking, etc\nMuch of the publicly available health data that can be used to train models that you may wish to deploy in production have licences that forbid commercial usage – make sure you check these constraints before committing to a project\nIf you are offering ML models to external clients and your models leverage open source data, then you will find that you don’t need to regularly retrain these models since you’ll most likely be using a static snapshot for a one-off training exercise. This simplifies model management\nFor IT implementation at partner/clinical sites, typically design, governance, and security sign-off will be required before people can be allocated to the work. This requires clear articulation of the benefits of the work in order to be prioritised. Ensure there is strong clinical engagement from the partner site to drive this see End-user Engagement and collaboration\nAlso refer to the Ministry of Health - Emerging Health Technology Advice & Guidance on operationalising algorithms"
  },
  {
    "objectID": "11-deployment-lifecycle.html#lifecycle-model-maintenance",
    "href": "11-deployment-lifecycle.html#lifecycle-model-maintenance",
    "title": "10  Deployment & lifecycle maintenance",
    "section": "10.2 Lifecycle model maintenance",
    "text": "10.2 Lifecycle model maintenance\n\n\n\nThe CRISP-DM Data Science Lifecycle\n\n\nShearer, C. (2000). The CRISP-DM model: the new blueprint for data mining. Journal of data warehousing, 5(4), 13-22.\nDeployment of a model is never the end of modelling work. During its continuing life cycle, the model and its working environment need to be overseen continuously. Some early questions to consider are:\n\nWhat is the governance process?\nWho is responsible for oversight and/or maintenance?\nWhat is the process for monitoring model drift?\nIs the model being used in the way it was originally intended according to the original use case?\nDo the intended users of the model understand, trust and use the model outputs?\n\nIn the case of “model drift” (non-negligible performance decrease is detected, or any condition for the model to properly work is no longer satisfied), the model and model application need to be reviewed, and in necessary cases the partial or whole modelling process should be reapplied to create a refreshed model.\nRefer also to ’explaining the outputs, transparency & interpretability\n\n10.2.1 Model refreshment\nThere are two main approaches for triggering model refreshment:\n\nTime based refreshment. Retrain the model (regardless of the performance) at a regular interval. Better quality data, more current data, data with better population coverage or new data sources may become available over time meaning that a model trained on updated data would likely have significant performance improvements over the existing model. A good understanding of how frequently the data changes is needed for this approach.\nPerformance based refreshment. Continuously monitor a set of the model performance metrics (and/or other metrics such as bias) to determine when the model needs a retraining. A good selection of the panel of metrics and thresholds is needed for this approach.\n\nBut there are other considerations for model refreshment including:\n\nAny change in how the model is being used. Are the users of the model still using it according to the original use case? If the model is being used for a different purpose, is it appropriate for that purpose?\nPotential for increased scrutiny of the model. You don’t want your model to be viewed negatively publicly and should consider how use of the model would look on the front page of the newspaper, particularly if there have been shifts in the inputs, in the outcomes, in the environment (e.g. new disease variants) or in attitudes that affect social licence.\n\n\n\n10.2.2 Model monitoring\nKey areas to be monitored during model use include:\n\nThe data feed. Are there any changes in the statistical properties of the data that are fed into the model, and the actual data labels or patient outcomes (there will be a lag to collect these) that the model predicts? Are they still similar to the data that the model was trained on? Has a “concept drift” detector been set up in the monitoring process? How is the bias in the data changing?\nThe model performance metrics and bias metrics. Is there any significant drop in the model performance or model fairness and what are the possible reasons to be taken into account for retraining? This could be when a given metric, such as accuracy or combination of metrics, such a sensitivity and specificity have dropped below a certain threshold, or there is a clear downward trend in those metrics.\nThe key assumptions that the modelling was based on. For example, in the COVID pandemic modelling work, is the current dominating virus variant still the same as the one when the modelling data were collected?\nThe workflow where the model is integrated. Are there any changes in the upstream or downstream steps of the workflow that may make the model not applicable any more?\nThe risk, benefit and cost of model usage. Does the benefit outweigh the risk in real use? Is the model use case as cost-effective as it was expected at design?\n\nMore can be done besides the list above. For a more comprehensive model monitoring and retraining techniques, refer to this blog: A Practical Guide to Maintaining Machine Learning in Production."
  },
  {
    "objectID": "12-references.html",
    "href": "12-references.html",
    "title": "References",
    "section": "",
    "text": "The NZ Algorithm Scan\nAI for Health in NZ report (AI Forum)\nTensorflow Extended Guide\nModel cards for Model Reporting\nDM-BOK (Data Management Body of Knowledge)\nLearning Spark (Book, comprehensive)\nA guide to good practice for digital and data-driven health technologies - GOV.UK\nALGORITHM ASSESSMENT REPORT\nResearch and engagement — Digital Identity Programme\nPrinciples for the safe and effective use of data and analytics\nAlgorithm charter for Aotearoa New Zealand - data.govt.nz\nGOVERNMENT USE of ARTIFICIAL INTELLIGENCE in NEW ZEALAND\nEmerging health technology - advice and guidance\nGuide for safely developing & using Algorithms in Healthcare\nThe Algorithm Charter | Ministry of Health NZ\nTrustworthy AI in Aotearoa AI Principles\nA guide to good practice for digital and data-driven health technologies\nData governance activities: an analysis of the literature\nThe Challenges of Big Data Governance in Healthcare\nData Ethics and Data Governance from A Māori World View (NZ)"
  },
  {
    "objectID": "13-glossary.html",
    "href": "13-glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "ML: {#glossary-ml} machine learning"
  }
]